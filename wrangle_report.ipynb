{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal is gathering, assessing, and cleaning the raw data for further analysis, ensuring the quality and tidiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources:\n",
    "\n",
    "1. `twitter-archive-enhanced.csv` dataset\n",
    "2. `image-predictions.tsv` from `https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv` url\n",
    "3. `tweet-json.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Details:\n",
    "1. Gathering Data\n",
    "2. Assessing Data\n",
    "3. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`twitter-archive-enhanced.csv` file: \n",
    "- Download the `twitter-archive-enhanced.csv` and then upload the dataset to the workspace.\n",
    "- Use the pandas read_csv() function to read the file into a dataframe named `twitter_df`.\n",
    "\n",
    "`image-predictions.tsv` file:\n",
    "- Get the \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\" url.\n",
    "- Import requests library and use the function to get the data from above url.\n",
    "- Use the content function to get the content of `image-predictions.tsv` and then write the content to a same name tsv file.\n",
    "- Use the pandas read_csv() with sep='\\t' argurment to read the file into a dataframe named `images_df`\n",
    "\n",
    "`tweet-json.txt`:\n",
    "- This file supposed to be created by the twitter API. But currently I am struggling with the API because of the updating from v1 to v2. For pushing the progress faster, I used the `tweet-json.txt` provided by the project guidline.\n",
    "- Uses the pandas read_json() function to read the `tweet-json.txt` file and put it in the dataframe named `tweets_df`.\n",
    "- Extract the `tweets_df` dataframe to get relevant columns: 'id', 'retweet_count', 'favorite_count'.\n",
    "- Rename the columns for clarity, change the names to 'tweet_id', 'retweet_count', 'favorite_count'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Assessment: <br>\n",
    "- Print three dataframes individually.\n",
    "- Learn about comprehensive meaning of the datasets.\n",
    "\n",
    "Programmatic Assessment: <br>\n",
    "- Identify mising values, outliers, inconsistency data, duplicated values ... using .describe(), .info(), .duplicated(), .value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Made a copy of the original data before cleaning:  `twitter_clean`, `images_clean`, `tweets_clean`.\n",
    "\n",
    "- Used the Define-Code-Test framework for cleaning each issue.\n",
    "\n",
    "Clean up the missing data issue first:\n",
    "- Remove all the retweets rows to make only original dog ratings for the analysis.\n",
    "- Remove the columns retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id and in_reply_to_user_id because not only is most of the data missing, but those columns are also irrelevant to project analytics\n",
    "\n",
    "- Create new column named `rate`: rate = rating_numerator divided by rating_denominator.\n",
    "\n",
    "Secondly is cleaning up the tidiness issue:\n",
    "\n",
    "- Combine the four dog stages columns into one single column.\n",
    "\n",
    "- Merge `twitter_clean`, `images_clean`, `tweets_clean` into one dataset named `Merge_df` using inner join on the `tweet_id`.\n",
    "\n",
    "Lastly is cleaning up the quality issue:\n",
    "\n",
    "- Fix the erroneous datatypes (timestamp should be a datetime, tweet_id should be a string).\n",
    "\n",
    "- Remove the html tag in source column.\n",
    "\n",
    "- Replace all cells containing names without actual names to \"None\".\n",
    "\n",
    "- Manually change the name of 776201521193218049 cell  from O to O'Malley.\n",
    "\n",
    "- Change the dog_stage datatype to \"category\".\n",
    "\n",
    "- Convert all values in `p1`, `p2`, `p3` to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data:\n",
    "\n",
    "Save the merged data in a csv file named `twitter_archive_master.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Transformed raw and disparate data into clen data for in-depth analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
